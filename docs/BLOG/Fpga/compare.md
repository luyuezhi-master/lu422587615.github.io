# FPGA 与 GPU 的对比

> 写于 230609

先明确几种硬件的定义：

- CPU：基于 ARM、X86 指令集架构的计算设备，一般一个周期只能处理一条指令（SISD），但是通用性极强。
- GPU：只考虑 NVIDIA 的 GPU，主要做计算机图形学渲染的任务，用于二维显示和三维场景渲染计算。特点是小的浮点核极多（SIMD）。
    - 深度学习加速卡：一种相对通用的加速神经网络训练推理的计算设备。典型有 GPGPU 和 TPU（Google）。主要的思路是通过硬件优化加速深度学习中的常用运算，如加速矩阵乘法、transformer 计算等。
    - GPGPU：将 GPU 除去显示部分，可以看作一个计算通用的 GPU。只考虑 NVIDIA 的 GPU，通过 CUDA 指令集使得开发者可以在 GPU 上执行一些通用的计算任务，如加速神经网络的训练。是一种深度学习加速卡。
    - 方便起见，我们下面所说的 GPU 统一指这一类深度学习加速卡。
- FPGA：有着逻辑单元和布线资源的可定制、可重用硬件。通过 HDL 可以对 FPGA 进行编程从而实现预期的硬件计算功能。FPGA 可以用于实现具有通用功能或特定功能的硬件。适合 SIMD 和 MISD 的计算任务。
- ASIC：专用电路，设计好逻辑计算方式后，流片不可更改。用途固定的芯片，如 CPU、GPU、FPGA、音视频编解码器这些都是 ASIC。设计的好性能极高，这里设计就是通俗说的「造芯」。

注：AI 芯片：AI 芯片这个名字太大了。具体一些可以分为两种，一种是类似 GPGPU 的有通用性的深度学习加速卡，另一种是目标为特定 AI 算法的小 ASIC（类似 DSP）。之后不会提及 AI 芯片这个名称。

在计算加速任务上，考虑 CPU GPU FPGA ASIC 这四种硬件。从左到右，从软件到硬件，通用性逐渐降低、越专用，可定制化逐渐提高，相应的设计/开发成本逐渐提高，但是单位成本理论性能越高。

对于一个计算任务来说：
可以通过时效性来选择计算平台：对于需要一周更新一次的网页来说，肯定使用最通用的 CPU 来提高开发和部署速度；对于还在实验室阶段的经典算法或深度学习算法，使用 CPU、GPU 做软件方面的探索很合适；对于已经逐渐成为标准的技术，使用 FPGA 做硬件加速部署；对于已经成为标准的计算任务，则直接推出专用芯片（H265 视频解码芯片、DSP）。
可以通过算力选择计算平台：CPU 不适合处理图像、视频、深度学习数据这种大批量的数据；GPU 浮点运算能力很强，适合高精度的神经网络计算；FPGA 并不擅长浮点运算，但是对于网络数据包、视频流可以做到很强的流水线处理；ASIC 则根据成本有几乎无限的算力，取决于硬件设计者。

从公司的角度来说：
同样对于大批量数据的计算任务，GPU 和 FPGA 相比，均衡来说，同等内存大小、同等算力的成熟 GPU 和 FPGA 的部署成本想近。
如果公司的业务逻辑经常变化（1-2年变化一次），GPU 的开发成本低、部署速度快；业务偶尔发生变化（5年左右变化一次），FPGA 开发成本虽高、但芯片本身的成本相比 GPU 低很多。
除此以外还需要考虑公司规模。GPU 开发偏软件开发，门槛较低，资源相对容易获取；FPGA 开发本质上属于硬件开发，门槛高，公司规模越大越容易做。

在当下这个科技发展极快、算法以月为单位更迭的大数据时代，GPU 确实适合更多人；但是一旦有一个商业需求固定下来，FPGA 甚至 ASIC 则会成为更好的底层计算设备。
